{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Assignment 1: Tokenization, Stemming, Lemmatization\n",
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, TreebankWordTokenizer, TweetTokenizer, MWETokenizer, word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "Mha1jLiph0bC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtQsxDBlgY3M",
        "outputId": "4c3908ca-d6c6-4fb3-bd0f-49fed5e932e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assignment 1: Tokenization, Stemming, Lemmatization\n",
            "Whitespace Tokenizer: ['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI.', 'NLP', 'enables', 'machines', 'to', 'understand', 'human', 'language!']\n",
            "Punctuation-based Tokenizer: ['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI', '.', 'NLP', 'enables', 'machines', 'to', 'understand', 'human', 'language', '!']\n",
            "Treebank Tokenizer: ['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI.', 'NLP', 'enables', 'machines', 'to', 'understand', 'human', 'language', '!']\n",
            "Tweet Tokenizer: ['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI', '.', 'NLP', 'enables', 'machines', 'to', 'understand', 'human', 'language', '!']\n",
            "MWE Tokenizer: ['Natural_Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI.', 'NLP', 'enables', 'machines', 'to', 'understand', 'human', 'language!']\n",
            "Porter Stemmer: ['natur', 'languag', 'process', 'is', 'an', 'excit', 'field', 'of', 'ai', '.', 'nlp', 'enabl', 'machin', 'to', 'understand', 'human', 'languag', '!']\n",
            "Snowball Stemmer: ['natur', 'languag', 'process', 'is', 'an', 'excit', 'field', 'of', 'ai', '.', 'nlp', 'enabl', 'machin', 'to', 'understand', 'human', 'languag', '!']\n",
            "Lemmatization: ['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'of', 'AI', '.', 'NLP', 'enables', 'machine', 'to', 'understand', 'human', 'language', '!']\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "document = \"Natural Language Processing is an exciting field of AI. NLP enables machines to understand human language!\"\n",
        "\n",
        "print(\"\\nAssignment 1: Tokenization, Stemming, Lemmatization\")\n",
        "print(\"Whitespace Tokenizer:\", WhitespaceTokenizer().tokenize(document))\n",
        "print(\"Punctuation-based Tokenizer:\", word_tokenize(document))\n",
        "print(\"Treebank Tokenizer:\", TreebankWordTokenizer().tokenize(document))\n",
        "print(\"Tweet Tokenizer:\", TweetTokenizer().tokenize(document))\n",
        "\n",
        "mwe_tokenizer = MWETokenizer([(\"Natural\", \"Language\"), (\"human\", \"language\")])\n",
        "print(\"MWE Tokenizer:\", mwe_tokenizer.tokenize(document.split()))\n",
        "\n",
        "ps = PorterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "print(\"Porter Stemmer:\", [ps.stem(word) for word in word_tokenize(document)])\n",
        "print(\"Snowball Stemmer:\", [snowball.stem(word) for word in word_tokenize(document)])\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
        "\n",
        "print(\"Lemmatization:\", lemmatize_text(document))"
      ]
    }
  ]
}